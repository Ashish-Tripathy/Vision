{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Asg 4 DNN 4th Iteration.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Ashish-Tripathy/ComputerVision/blob/master/Asg_4_DNN_4th_Iteration.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jDL8XoclLuE7",
        "colab_type": "text"
      },
      "source": [
        "# Architectural Highlights of this Iteration\n",
        "1. For the 4th iteration, I introduce dropout after each convolution layer.\n",
        "2. I will also increase the batch size to 128, this will decrease the time taken for each epoch and also push the accuracy further"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aNyZv-Ec52ot",
        "colab_type": "text"
      },
      "source": [
        "# **Import Libraries and modules**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3m3w1Cw49Zkt",
        "colab_type": "code",
        "outputId": "fd6d8313-aae1-42cc-bd2c-5cbea224c635",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "\n",
        "import keras"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Using TensorFlow backend.\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Eso6UHE080D4",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import numpy as np\n",
        "\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Dense, Dropout, Activation, Flatten, Add\n",
        "from keras.layers import Convolution2D, MaxPooling2D\n",
        "from keras.utils import np_utils\n",
        "\n",
        "from keras.datasets import mnist"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zByEi95J86RD",
        "colab_type": "text"
      },
      "source": [
        "### Load pre-shuffled MNIST data into train and test sets"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7eRM0QWN83PV",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "(X_train, y_train), (X_test, y_test) = mnist.load_data()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4a4Be72j8-ZC",
        "colab_type": "code",
        "outputId": "e580b779-a2cd-4993-9048-ad71ebaa9f6f",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 304
        }
      },
      "source": [
        "print (X_train.shape)\n",
        "from matplotlib import pyplot as plt\n",
        "%matplotlib inline\n",
        "plt.imshow(X_train[2])"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(60000, 28, 28)\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<matplotlib.image.AxesImage at 0x7f586bbb6da0>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 4
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAP8AAAD8CAYAAAC4nHJkAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dkmprriw9AnZ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "X_train = X_train.reshape(X_train.shape[0], 28, 28,1)\n",
        "X_test = X_test.reshape(X_test.shape[0], 28, 28,1)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "X2m4YS4E9CRh",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "X_train = X_train.astype('float32')\n",
        "X_test = X_test.astype('float32')\n",
        "X_train /= 255\n",
        "X_test /= 255"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0Mn0vAYD9DvB",
        "colab_type": "code",
        "outputId": "ee42014f-a4c9-4f02-da22-bfce63585e14",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "y_train[:10]"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([5, 0, 4, 1, 9, 2, 1, 3, 1, 4], dtype=uint8)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZG8JiXR39FHC",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Convert 1-dimensional class arrays to 10-dimensional class matrices\n",
        "Y_train = np_utils.to_categorical(y_train, 10)\n",
        "Y_test = np_utils.to_categorical(y_test, 10)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fYlFRvKS9HMB",
        "colab_type": "code",
        "outputId": "ac7e60ed-d7c1-4640-a880-d13f3d9a2943",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 195
        }
      },
      "source": [
        "Y_train[:10]\n"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[0., 0., 0., 0., 0., 1., 0., 0., 0., 0.],\n",
              "       [1., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
              "       [0., 0., 0., 0., 1., 0., 0., 0., 0., 0.],\n",
              "       [0., 1., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
              "       [0., 0., 0., 0., 0., 0., 0., 0., 0., 1.],\n",
              "       [0., 0., 1., 0., 0., 0., 0., 0., 0., 0.],\n",
              "       [0., 1., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
              "       [0., 0., 0., 1., 0., 0., 0., 0., 0., 0.],\n",
              "       [0., 1., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
              "       [0., 0., 0., 0., 1., 0., 0., 0., 0., 0.]], dtype=float32)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1WTKtmRVlWYS",
        "colab_type": "text"
      },
      "source": [
        "# Building the Architecture"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dGWcXXd6a6vY",
        "colab_type": "code",
        "outputId": "66fbdd87-fb36-4798-bd0b-078e0e09aae2",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1279
        }
      },
      "source": [
        "from keras.models import Model\n",
        "from keras.layers import Input,Conv2D, BatchNormalization,Activation, Flatten\n",
        "\n",
        "input = Input(shape=(28, 28, 1,))\n",
        "\n",
        "#defining convolution block\n",
        "def conv_block(inputs, filters,padding = 'valid'):\n",
        "  conv = Conv2D(filters = filters, kernel_size = (3,3), padding=padding, use_bias=False)(inputs)\n",
        "  conv = Dropout(0.1)(conv)\n",
        "  conv = BatchNormalization()(conv)\n",
        "  conv = Activation('relu')(conv)\n",
        "  return conv\n",
        "\n",
        "#defining transition block\n",
        "def bottleneck(inputs,filters):\n",
        "  conv = Conv2D(filters = filters, kernel_size = (1,1), padding = 'valid', use_bias = False)(inputs)\n",
        "  conv = Dropout(0.1)(conv)\n",
        "  conv = BatchNormalization()(conv)\n",
        "  conv = Activation('relu')(conv)\n",
        "  return MaxPooling2D()(conv)\n",
        "\n",
        "#Building the architecture\n",
        "First_Layer  =  conv_block(input, 8) #26 RF 3x3\n",
        "Second_Layer =  conv_block(First_Layer,12) #24 RF 5x5\n",
        "Third_Layer  =  conv_block(Second_Layer,16) #22 RF 5x5\n",
        "Transition_1 =  bottleneck(Third_Layer,12) #11 RF 7x7\n",
        "Fourth_Layer =  conv_block(Transition_1,16) #9 RF 14x14\n",
        "Fifth_Layer =  conv_block(Fourth_Layer,32)  #7 RF 16x16 \n",
        "last1x1      =  Conv2D(filters = 10, kernel_size = (1,1), padding = 'valid', use_bias = False)(Fifth_Layer)\n",
        "last1x1      =  Activation('relu')(last1x1)\n",
        "last_layer   =  Conv2D(filters = 10, kernel_size = (7,7), padding = 'valid', use_bias = False)(last1x1) #1; RF 22x22\n",
        "last_layer   =  BatchNormalization()(last_layer)\n",
        "flatten      =  Flatten()(last_layer)\n",
        "output       =  Activation('softmax')(flatten)\n",
        "\n",
        "#creating the model\n",
        "model = Model(inputs=[input], outputs=[output])\n",
        "model.summary()\n",
        "\n",
        "\n",
        "\n"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "input_2 (InputLayer)         (None, 28, 28, 1)         0         \n",
            "_________________________________________________________________\n",
            "conv2d_9 (Conv2D)            (None, 26, 26, 8)         72        \n",
            "_________________________________________________________________\n",
            "dropout_7 (Dropout)          (None, 26, 26, 8)         0         \n",
            "_________________________________________________________________\n",
            "batch_normalization_8 (Batch (None, 26, 26, 8)         32        \n",
            "_________________________________________________________________\n",
            "activation_9 (Activation)    (None, 26, 26, 8)         0         \n",
            "_________________________________________________________________\n",
            "conv2d_10 (Conv2D)           (None, 24, 24, 12)        864       \n",
            "_________________________________________________________________\n",
            "dropout_8 (Dropout)          (None, 24, 24, 12)        0         \n",
            "_________________________________________________________________\n",
            "batch_normalization_9 (Batch (None, 24, 24, 12)        48        \n",
            "_________________________________________________________________\n",
            "activation_10 (Activation)   (None, 24, 24, 12)        0         \n",
            "_________________________________________________________________\n",
            "conv2d_11 (Conv2D)           (None, 22, 22, 16)        1728      \n",
            "_________________________________________________________________\n",
            "dropout_9 (Dropout)          (None, 22, 22, 16)        0         \n",
            "_________________________________________________________________\n",
            "batch_normalization_10 (Batc (None, 22, 22, 16)        64        \n",
            "_________________________________________________________________\n",
            "activation_11 (Activation)   (None, 22, 22, 16)        0         \n",
            "_________________________________________________________________\n",
            "conv2d_12 (Conv2D)           (None, 22, 22, 12)        192       \n",
            "_________________________________________________________________\n",
            "dropout_10 (Dropout)         (None, 22, 22, 12)        0         \n",
            "_________________________________________________________________\n",
            "batch_normalization_11 (Batc (None, 22, 22, 12)        48        \n",
            "_________________________________________________________________\n",
            "activation_12 (Activation)   (None, 22, 22, 12)        0         \n",
            "_________________________________________________________________\n",
            "max_pooling2d_2 (MaxPooling2 (None, 11, 11, 12)        0         \n",
            "_________________________________________________________________\n",
            "conv2d_13 (Conv2D)           (None, 9, 9, 16)          1728      \n",
            "_________________________________________________________________\n",
            "dropout_11 (Dropout)         (None, 9, 9, 16)          0         \n",
            "_________________________________________________________________\n",
            "batch_normalization_12 (Batc (None, 9, 9, 16)          64        \n",
            "_________________________________________________________________\n",
            "activation_13 (Activation)   (None, 9, 9, 16)          0         \n",
            "_________________________________________________________________\n",
            "conv2d_14 (Conv2D)           (None, 7, 7, 32)          4608      \n",
            "_________________________________________________________________\n",
            "dropout_12 (Dropout)         (None, 7, 7, 32)          0         \n",
            "_________________________________________________________________\n",
            "batch_normalization_13 (Batc (None, 7, 7, 32)          128       \n",
            "_________________________________________________________________\n",
            "activation_14 (Activation)   (None, 7, 7, 32)          0         \n",
            "_________________________________________________________________\n",
            "conv2d_15 (Conv2D)           (None, 7, 7, 10)          320       \n",
            "_________________________________________________________________\n",
            "activation_15 (Activation)   (None, 7, 7, 10)          0         \n",
            "_________________________________________________________________\n",
            "conv2d_16 (Conv2D)           (None, 1, 1, 10)          4900      \n",
            "_________________________________________________________________\n",
            "batch_normalization_14 (Batc (None, 1, 1, 10)          40        \n",
            "_________________________________________________________________\n",
            "flatten_2 (Flatten)          (None, 10)                0         \n",
            "_________________________________________________________________\n",
            "activation_16 (Activation)   (None, 10)                0         \n",
            "=================================================================\n",
            "Total params: 14,836\n",
            "Trainable params: 14,624\n",
            "Non-trainable params: 212\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Zp6SuGrL9M3h",
        "colab_type": "code",
        "outputId": "69563155-0107-455d-a3c7-eb8482b8dab9",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 781
        }
      },
      "source": [
        "from keras.callbacks import *\n",
        "\n",
        "model.compile(loss='categorical_crossentropy',\n",
        "             optimizer='adam',\n",
        "             metrics=['accuracy'])\n",
        "\n",
        "model.fit(X_train, Y_train, validation_data=(X_test, Y_test), nb_epoch = 20, verbose = 1, batch_size = 128)"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:7: UserWarning: The `nb_epoch` argument in `fit` has been renamed `epochs`.\n",
            "  import sys\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Train on 60000 samples, validate on 10000 samples\n",
            "Epoch 1/20\n",
            "60000/60000 [==============================] - 7s 121us/step - loss: 0.6178 - acc: 0.8882 - val_loss: 0.2799 - val_acc: 0.9640\n",
            "Epoch 2/20\n",
            "60000/60000 [==============================] - 6s 100us/step - loss: 0.2367 - acc: 0.9699 - val_loss: 0.1447 - val_acc: 0.9816\n",
            "Epoch 3/20\n",
            "60000/60000 [==============================] - 6s 101us/step - loss: 0.1511 - acc: 0.9789 - val_loss: 0.0920 - val_acc: 0.9863\n",
            "Epoch 4/20\n",
            "60000/60000 [==============================] - 6s 103us/step - loss: 0.1113 - acc: 0.9836 - val_loss: 0.0709 - val_acc: 0.9869\n",
            "60000/60000 [==============================] - 7s 109us/step - loss: 0.0897 - acc: 0.9842 - val_loss: 0.0514 - val_acc: 0.9889\n",
            "Epoch 6/20\n",
            "60000/60000 [==============================] - 6s 102us/step - loss: 0.0724 - acc: 0.9869 - val_loss: 0.0428 - val_acc: 0.9916\n",
            "Epoch 7/20\n",
            "60000/60000 [==============================] - 6s 100us/step - loss: 0.0631 - acc: 0.9877 - val_loss: 0.0457 - val_acc: 0.9911\n",
            "Epoch 8/20\n",
            "60000/60000 [==============================] - 6s 101us/step - loss: 0.0553 - acc: 0.9885 - val_loss: 0.0341 - val_acc: 0.9925\n",
            "Epoch 9/20\n",
            "60000/60000 [==============================] - 6s 100us/step - loss: 0.0489 - acc: 0.9896 - val_loss: 0.0320 - val_acc: 0.9919\n",
            "Epoch 10/20\n",
            "60000/60000 [==============================] - 6s 98us/step - loss: 0.0452 - acc: 0.9898 - val_loss: 0.0289 - val_acc: 0.9942\n",
            "Epoch 11/20\n",
            "60000/60000 [==============================] - 6s 99us/step - loss: 0.0420 - acc: 0.9902 - val_loss: 0.0309 - val_acc: 0.9933\n",
            "Epoch 12/20\n",
            "60000/60000 [==============================] - 6s 98us/step - loss: 0.0383 - acc: 0.9909 - val_loss: 0.0259 - val_acc: 0.9937\n",
            "Epoch 13/20\n",
            "60000/60000 [==============================] - 6s 98us/step - loss: 0.0360 - acc: 0.9912 - val_loss: 0.0258 - val_acc: 0.9933\n",
            "Epoch 14/20\n",
            "60000/60000 [==============================] - 6s 98us/step - loss: 0.0346 - acc: 0.9913 - val_loss: 0.0250 - val_acc: 0.9927\n",
            "Epoch 15/20\n",
            "60000/60000 [==============================] - 6s 98us/step - loss: 0.0319 - acc: 0.9920 - val_loss: 0.0233 - val_acc: 0.9934\n",
            "Epoch 16/20\n",
            "60000/60000 [==============================] - 6s 100us/step - loss: 0.0306 - acc: 0.9920 - val_loss: 0.0217 - val_acc: 0.9942\n",
            "Epoch 17/20\n",
            "60000/60000 [==============================] - 6s 99us/step - loss: 0.0296 - acc: 0.9923 - val_loss: 0.0251 - val_acc: 0.9923\n",
            "Epoch 18/20\n",
            "60000/60000 [==============================] - 7s 109us/step - loss: 0.0270 - acc: 0.9926 - val_loss: 0.0226 - val_acc: 0.9932\n",
            "Epoch 19/20\n",
            "60000/60000 [==============================] - 6s 105us/step - loss: 0.0275 - acc: 0.9920 - val_loss: 0.0238 - val_acc: 0.9928\n",
            "Epoch 20/20\n",
            "60000/60000 [==============================] - 6s 99us/step - loss: 0.0250 - acc: 0.9934 - val_loss: 0.0281 - val_acc: 0.9916\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.callbacks.History at 0x7f583b5b9cf8>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hBEuKDp7ApXD",
        "colab_type": "text"
      },
      "source": [
        "# Observations\n",
        "1. <b>Best Validation Accuracy: 99.42% at 10th Epoch; Parameters: 14.8K; </b>\n",
        "2. The desired accuracy of 99.42% was achieved by the 10th epoch itself. We can see adding dropout for every layer generalises the model even more helping achieving better accuracy in lesser epochs\n",
        "3. Increasing batch size reduced the per epoch time from average of 20 seconds to just 6 seconds\n",
        "\n"
      ]
    }
  ]
}